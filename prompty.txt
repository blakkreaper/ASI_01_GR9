"detail":"[36mray::_sub_fit()[39m (pid=8588, ip=127.0.0.1)
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "C:\ProgramData\Anaconda3\envs\ASI2\lib\site-packages\autogluon\tabular\predictor\predictor.py", line 4971, in _sub_fit
    train_data, val_data = generate_train_test_split_combined(
  File "C:\ProgramData\Anaconda3\envs\ASI2\lib\site-packages\autogluon\core\utils\utils.py", line 394, in generate_train_test_split_combined
    train_data, test_data, y_train, y_test = generate_train_test_split(
  File "C:\ProgramData\Anaconda3\envs\ASI2\lib\site-packages\autogluon\core\utils\utils.py", line 478, in generate_train_test_split
    X_train, X_test, y_train, y_test = train_test_split(
  File "C:\ProgramData\Anaconda3\envs\ASI2\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\ProgramData\Anaconda3\envs\ASI2\lib\site-packages\sklearn\model_selection\_split.py", line 2683, in train_test_split
    return list(
  File "C:\ProgramData\Anaconda3\envs\ASI2\lib\site-packages\sklearn\model_selection\_split.py", line 2685, in <genexpr>
    (_safe_indexing(a, train), _safe_indexing(a, test)) for a in arrays
  File "C:\ProgramData\Anaconda3\envs\ASI2\lib\site-packages\sklearn\utils\__init__.py", line 411, in _safe_indexing
    return _array_indexing(X, indices, indices_dtype, axis=axis)
  File "C:\ProgramData\Anaconda3\envs\ASI2\lib\site-packages\sklearn\utils\__init__.py", line 208, in _array_indexing
    return array[key, ...] if axis == 0 else array[:, key]
  File "C:\ProgramData\Anaconda3\envs\ASI2\lib\site-packages\pandas\core\arrays\arrow\array.py", line 618, in __getitem__
    value = self._pa_array[item]
  File "pyarrow\\table.pxi", line 305, in pyarrow.lib.ChunkedArray.__getitem__
TypeError: only integer scalar arrays can be converted to a scalar index"

from autogluon.tabular import TabularPredictor

hyperparameters = {
    'NN': {'num_epochs': 10, 'learning_rate': 1e-3, 'dropout_prob': 0.1},
    'GBM': {'num_boost_round': 100, 'learning_rate': 0.05, 'num_leaves': 31},
    'CAT': {'iterations': 500, 'depth': 6, 'learning_rate': 0.1},
    'XGB': {'n_estimators': 100, 'learning_rate': 0.1, 'max_depth': 6},
    'RF': {'n_estimators': 100, 'max_depth': 15},
    'XT': {'n_estimators': 100, 'max_depth': 15},
    'KNN': {'n_neighbors': 5, 'weights': 'uniform', 'algorithm': 'auto'},
    'LR': {},
    'FASTAI': {'epochs': 10, 'lr': 1e-2}
}

label = 'your_label_column'
train_data = your_dataframe

predictor = TabularPredictor(label=label, eval_metric='accuracy').fit(
    train_data=train_data,
    hyperparameters=hyperparameters,
    presets='best_quality',
    time_limit=3600
)

hyperparameters = {
    'NN': {
        'num_epochs': 10,
        'learning_rate': 1e-3,
        'dropout_prob': 0.1,
    },
    'GBM': {
        'num_boost_round': 100,
        'learning_rate': 0.05,
        'num_leaves': 31,
    },
    'CAT': {
        'iterations': 500,
        'depth': 6,
        'learning_rate': 0.1,
    },
    'XGB': {
        'n_estimators': 100,
        'learning_rate': 0.1,
        'max_depth': 6,
    },
    'RF': {
        'n_estimators': 100,
        'max_depth': 15,
    },
    'XT': {
        'n_estimators': 100,
        'max_depth': 15,
    },
    'KNN': {
        'n_neighbors': 5,
        'weights': 'uniform',
        'algorithm': 'auto',
    },
    'LR': {},
    'FASTAI': {
        'epochs': 10,
        'lr': 1e-2,
    }
}


Oto przeglƒÖd twoich hiperparametr√≥w z uwzglƒôdnieniem ich zgodno≈õci z odpowiednimi modelami:

XGBoost (XGB):

booster: Warto≈õƒá 'gbtree' jest poprawna.
verbosity: Warto≈õƒá 1 jest poprawna (0: silent, 1: warning, 2: info, 3: debug).
LightGBM (GBM):

extra_trees: Warto≈õƒá True jest poprawna, chocia≈º nie jest to typowy hiperparametr w LightGBM. Mo≈ºe to byƒá niestandardowa opcja, ale warto to sprawdziƒá w dokumentacji LightGBM.
Random Forest (RF):

n_estimators: Warto≈õƒá 100 jest poprawna.
max_depth: Warto≈õƒá 10 jest poprawna.
K-Nearest Neighbors (KNN):

weights: Warto≈õƒá 'uniform' jest poprawna.
n_neighbors: Warto≈õƒá 5 jest poprawna.
CatBoost (CAT):

iterations: Warto≈õƒá 10000 jest poprawna, choƒá do≈õƒá wysoka. Mo≈ºe warto zaczƒÖƒá od mniejszej warto≈õci, np. 1000, aby skr√≥ciƒá czas trenowania.
learning_rate: Warto≈õƒá 0.01 jest poprawna.
Uwaga na temat extra_trees w LightGBM
extra_trees mo≈ºe nie byƒá standardowym hiperparametrem w LightGBM. Zwykle LightGBM ma parametry takie jak boosting_type, num_leaves, learning_rate, itp. Sprawd≈∫, czy extra_trees jest rzeczywi≈õcie wspieranym parametrem w wersji LightGBM, kt√≥rej u≈ºywasz. Mo≈ºe to byƒá niestandardowa opcja dodana przez zesp√≥≈Ç AutoGluon.

Propozycja dla LightGBM
Je≈õli extra_trees nie jest wspieranym parametrem, mo≈ºesz rozwa≈ºyƒá u≈ºycie innych popularnych hiperparametr√≥w:
'GBM': {'boosting_type': 'gbdt', 'num_leaves': 31, 'learning_rate': 0.05, 'n_estimators': 100}



------------------------------------------------------------------------------------------
def train_model_and_evaluate(train_data: dd.DataFrame, test_data: dd.DataFrame) -> TabularPredictor:
    # Konwersja danych z Dask DataFrame na Pandas DataFrame
    train_data: pd.DataFrame = train_data.compute()
    test_data: pd.DataFrame = test_data.compute()

    # Sprawdzenie typ√≥w danych
    print("Train data type:", type(train_data))
    print("Test data type:", type(test_data))

    # Konwersja na Pandas DataFrame, je≈õli dane sƒÖ w formacie pyarrow
    if isinstance(train_data, pd.DataFrame):
        train_data = train_data.convert_dtypes()

    if isinstance(test_data, pd.DataFrame):
        test_data = test_data.convert_dtypes()

    # ≈öcie≈ºka do pliku z hiperparametrami
    hyperparams_path = "data/05_model_input/hyperparameters.json"
    
    # Domy≈õlne hiperparametry
    default_hyperparameters = {
        'GBM': {'extra_trees': True},
        'RF': {'n_estimators': 100, 'max_depth': 10},
        'KNN': {'weights': 'uniform', 'n_neighbors': 5},
        'CAT': {'iterations': 10000, 'learning_rate': 0.01},
        'XGB': {'booster': 'gbtree', 'verbosity': 1}
    }
    
    # Sprawdzenie, czy plik istnieje i za≈Çadowanie hiperparametr√≥w
    if os.path.exists(hyperparams_path):
        with open(hyperparams_path, "r") as f:
            loaded_hyperparameters = json.load(f)
            
        # Uzupe≈Çnienie brakujƒÖcych warto≈õci domy≈õlnymi
        hyperparameters = {}
        for key, default_params in default_hyperparameters.items():
            loaded_params = loaded_hyperparameters.get(key, {})
            hyperparameters[key] = {param: loaded_params.get(param, default_value) for param, default_value in default_params.items()}
    else:
        hyperparameters = default_hyperparameters

    # Inicjalizacja sesji WANDB
    print(hyperparameters)
    wandb.init(project="depression_prediction", entity="s22807")
    label: str = 'Class'
    
    # Trenowanie modelu z AutoGluon
    predictor: TabularPredictor = TabularPredictor(label=label, eval_metric='accuracy').fit(
        train_data,
        hyperparameters=hyperparameters,
        presets='best_quality',
        time_limit=3600
    )
    
    # Ewaluacja modelu
    performance: dict = predictor.evaluate(test_data)
    y_pred: np.ndarray = predictor.predict(test_data)
    y_score: np.ndarray = predictor.predict_proba(test_data)
    
    # Przygotowanie danych do wizualizacji w WANDB
    data_for_plot = [[metric, value] for metric, value in performance.items()]
    table = wandb.Table(data=data_for_plot, columns=["Metric", "Value"])
    
    # Logowanie wynik√≥w do WANDB
    wandb.log({"Evaluation Metrics": table})
    
    # Logowanie wykresu przy u≈ºyciu wandb.plot.bar
    wandb.log({"Metrics Bar Chart": wandb.plot.bar(table, "Metric", "Value", title="Performance Metrics")})
    
    # Logowanie krzywej ROC
    wandb.log({"ROC Curve": wandb.plot.roc_curve(test_data[label].values, y_score)})
    
    # Generowanie i logowanie macierzy omy≈Çek
    cm = confusion_matrix(test_data[label], y_pred)
    cm_display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=predictor.class_labels)
    
    fig, ax = plt.subplots(figsize=(10, 10))
    cm_display.plot(ax=ax)
    plt.title("Confusion Matrix")
    wandb.log({"Confusion Matrix": wandb.Image(fig)})
    
    # Zako≈Ñczenie sesji WANDB
    wandb.finish()
    
    # Tabela ze statystykami algorytm√≥w
    print(predictor.leaderboard())
    
    return predictor
